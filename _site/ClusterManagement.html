<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Cluster management in R</title>
  <meta name="description" content="Population-based detection of Copy-Number Variation from high-througput sequencing.
">
  <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:600' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/ClusterManagement.html">
  <link rel="alternate" type="application/rss+xml" title="PopSV" href="/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">PopSV</a>

    <nav class="site-nav">
        
          
          <a class="page-link" href="/BasicWorkflow.html">Analysis steps</a>
          
        
          
          <a class="page-link" href="/ClusterManagement.html">Cluster management in R</a>
          
        
          
          <a class="page-link" href="/Visualization.html">Visualization</a>
          
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
    </nav>

  </div>

</header>


    <div class="page-content">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Cluster management in R</h1>
  </header>

  <article class="post-content">
    <h2 id="batchjobs-package"><em>BatchJobs</em> package</h2>

<p><em>BatchJobs</em> is a potent package to communicate with a cluster, i.e. send jobs, check their status, eventually rerun, retrieve the results.
PopSV has been designed into separate steps to be ran on a computing cluster using <em>BatchJobs</em>.</p>

<p>A one-command version is also available there. It&#39;s basically a wrapper for the basic analysis steps with some useful functions (running custom steps, stop/restart). However it&#39;s less flexible.</p>

<h2 id="installation-and-configuration">Installation and configuration</h2>

<p>The package can be installed through CRAN</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">install.packages<span class="p">(</span><span class="s">&quot;BatchJobs&quot;</span><span class="p">)</span>
</code></pre></div>
<p>The most important part about <em>BatchJobs</em> is its configuration for your computing cluster. It&#39;s not long but should be done carefully. And once this is done correctly, the rest follows nicely.</p>

<p>You will need to create <strong>3 files</strong> : a cluster template, parser functions, and a <code>.BatchJobs.R</code> configuration file. I would recommend to put these 3 files <strong>in the root of your personal space</strong>, i.e. <code>~/</code>. You could put them in the project folder, but then it means you have to copy them each time you create/run another project. Putting them in your root means that they will be used by default by <em>BatchJobs</em>.</p>

<h3 id="cluster-template">Cluster template</h3>

<p>A cluster template is a template form of a job bash script that you would send through <code>qsub</code>/<code>msub</code>. There you define the placeholder for the resources or parameters of the job. This file will be parsed by <em>BatchJobs</em>. </p>

<p>For example, we create a <code>guillimin.tmpl</code> file for our cluster <a href="http://www.hpc.mcgill.ca/">Guillimin</a> like this :</p>
<div class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="c">#PBS -N &lt;%= job.name %&gt;</span>
<span class="c">#PBS -j oe</span>
<span class="c">#PBS -o &lt;%= log.file %&gt;</span>
<span class="c">#PBS -l walltime=&lt;%= resources$walltime %&gt;</span>
<span class="c">#PBS -l nodes=&lt;%= resources$nodes %&gt;:ppn=&lt;%= resources$cores %&gt;</span>
<span class="c">#PBS -A &lt;%= resources$supervisor.group %&gt;</span>
<span class="c">#PBS -q &lt;%= resources$queue %&gt;</span>
<span class="c">#PBS -V</span>

<span class="c">## Run R:</span>
<span class="c">## we merge R output with stdout from PBS, which gets then logged via -o option</span>
R CMD BATCH --no-save --no-restore <span class="s2">&quot;&lt;%= rscript %&gt;&quot;</span> /dev/stdout
</code></pre></div>
<p>Placeholders are in the form of <code>&lt;%= resources$walltime %&gt;</code>. <em>BatchJobs</em> will insert there the element define by <code>walltime</code> element in the <code>resources</code> list (see later). Although you most likely won&#39;t have to change these placeholders, you might need to update the lines if your cluster uses a different syntax. </p>

<h3 id="parser-functions">Parser functions</h3>

<p>Parser functions are saved in a R script, called for example <code>makeClusterFunctionsAdaptive.R</code>. This will parse the template and create the actual commands to send, cancel and check jobs.</p>

<p>Likely you just need to check/replace <code>qsub</code>/<code>qdel</code>/<code>qstat</code> calls with the correct bash commands (sometimes <code>msub</code>/<code>canceljob</code>/<code>showq</code>).</p>

<p>From our file, these are the lines you might need to change :</p>
<div class="highlight"><pre><code class="language-sh" data-lang="sh"><span class="nv">res</span> <span class="o">=</span>  BatchJobs:::runOSCommandLinux<span class="o">(</span><span class="s2">&quot;qsub&quot;</span>, outfile, stop.on.exit.code <span class="o">=</span> FALSE<span class="o">)</span>
cfKillBatchJob<span class="o">(</span><span class="s2">&quot;canceljob&quot;</span>, batch.job.id<span class="o">)</span>
BatchJobs:::runOSCommandLinux<span class="o">(</span><span class="s2">&quot;showq&quot;</span>, <span class="s2">&quot;-u $USER&quot;</span><span class="o">)</span><span class="nv">$output</span>
</code></pre></div>
<h3 id="batchjobs-r-configuration-file"><code>.BatchJobs.R</code> configuration file</h3>

<p><code>.BatchJobs.R</code>  is just the configuration files that links the two other files. You don&#39;t really need to change it. Eventually change the email address.</p>

<p>It looks like this :</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="kn">source</span><span class="p">(</span><span class="s">&quot;~/makeClusterFunctionsAdaptive.R&quot;</span><span class="p">)</span>
cluster.functions <span class="o">&lt;-</span> makeClusterFunctionsAdaptive<span class="p">(</span><span class="s">&quot;~/guillimin.tmpl&quot;</span><span class="p">)</span>
mail.start <span class="o">&lt;-</span> <span class="s">&quot;none&quot;</span>
mail.done <span class="o">&lt;-</span> <span class="s">&quot;none&quot;</span>
mail.error <span class="o">&lt;-</span> <span class="s">&quot;none&quot;</span>
mail.from <span class="o">&lt;-</span> <span class="s">&quot;&lt;jean.monlong@mail.mcgill.ca&gt;&quot;</span>
mail.to <span class="o">&lt;-</span> <span class="s">&quot;&lt;jean.monlong@mail.mcgill.ca&gt;&quot;</span>
</code></pre></div>
<p><em>Note: If <code>.BatchJobs.R</code> files are present at both <code>~/</code> and the project folder, the one in the project folder will override the parameters.</em></p>

<h2 id="sending-jobs">Sending Jobs</h2>

<p>In practice, <strong>you won&#39;t have to write this part</strong> as we provide full pipelines. You might still need to change a bit the resources of the jobs (they might change from one cluster to another). More precisely I&#39;m talking about the <code>resource=</code> parameter in the <code>submitJobs</code> command. After doing this, if you are not interested in more details, you can jump directly to the next section for an overview of a pipeline script.</p>

<p>Here is a quick summary of <em>BatchJobs</em> commands used in the scripts:</p>

<ul>
<li><code>makeRegistry</code> creates a registry used to manipulate jobs for a particular analysis step.</li>
<li><code>batchMap</code> adds jobs to a registry. Simply, you give it a function and a list of parameters. One job per parameter will be created to compute the output of the function using this specific parameter.</li>
<li><code>submitJobs</code> submits the jobs to the cluster. This is where the queue, maximum computation time, number of cores can be specified. Moreover, if needed, a subset of the jobs can be sent to the cluster. Functions <code>findNotDone</code> and <code>findErrors</code> are particularly useful to find which jobs didn&#39;t finish or were lost in the limbo of the cluster management process.</li>
<li><code>showStatus</code> outputs the status of the computations.</li>
<li><code>loadResult</code> retrieves the output of one specific job, while <code>reduceResultsList</code> retrieves output for all jobs into a list format.</li>
</ul>
<div class="highlight"><pre><code class="language-r" data-lang="r">getBC.reg <span class="o">&lt;-</span> makeRegistry<span class="p">(</span>id<span class="o">=</span><span class="s">&quot;getBC&quot;</span><span class="p">)</span>
getBC.f <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>file.i<span class="p">,</span> gc.reg<span class="p">,</span> files.df<span class="p">){</span>
  <span class="kn">library</span><span class="p">(</span>PopSV<span class="p">)</span>
  bins.df <span class="o">=</span> loadResult<span class="p">(</span>gc.reg<span class="p">,</span><span class="m">1</span><span class="p">)</span>
  bin.bam<span class="p">(</span>files.df<span class="o">$</span>bam<span class="p">[</span>file.i<span class="p">],</span> bins.df<span class="p">,</span> files.df<span class="o">$</span>bc<span class="p">[</span>file.i<span class="p">])</span>
<span class="p">}</span>
batchMap<span class="p">(</span>getBC.reg<span class="p">,</span> getBC.f<span class="p">,</span><span class="m">1</span><span class="o">:</span><span class="kp">nrow</span><span class="p">(</span>files.df<span class="p">),</span> more.args<span class="o">=</span><span class="kt">list</span><span class="p">(</span>gc.reg<span class="o">=</span>getGC.reg<span class="p">,</span> files.df<span class="o">=</span>files.df<span class="p">))</span>
submitJobs<span class="p">(</span>getBC.reg<span class="p">,</span> findNotDone<span class="p">(</span>getBC.reg<span class="p">),</span> resources<span class="o">=</span><span class="kt">list</span><span class="p">(</span>walltime<span class="o">=</span><span class="s">&quot;20:0:0&quot;</span><span class="p">,</span> nodes<span class="o">=</span><span class="s">&quot;1&quot;</span><span class="p">,</span> cores<span class="o">=</span><span class="s">&quot;1&quot;</span><span class="p">),</span> wait<span class="o">=</span><span class="kr">function</span><span class="p">(</span>retries<span class="p">)</span> <span class="m">100</span><span class="p">,</span> max.retries<span class="o">=</span><span class="m">10</span><span class="p">)</span>
</code></pre></div>
<p>Here we want to get the bin counts of each sample. We create a registry called <em>getBC</em>. Then the function that will get the bin counts for a sample. Here <code>file.i</code> is the index of the sample and will be different for each job sent by <em>BatchJobs</em>. The other parameters are common to all jobs. Within the function, we load the package and useful data and run the function we want. <code>batchMap</code> creates a job per sample id and link the function we just defined. The jobs are finally submitted to the cluster with specific number of cores, wall time, ... </p>

<p>We can check the status of the job with <code>showStatus(getBC.reg)</code> command. </p>

<h2 id="pipeline-worflow">Pipeline worflow</h2>

<p>The general idea is to have one script per analysis (e.g. bin size, project). Examples of pipeline scripts can be found in the <code>scripts</code> folder of the GitHub repository.</p>

<p>The script doesn&#39;t need to be ran each time from the start but rather ran step by step, most likely at separate times. Think about R as a new shell: in R the status of the jobs in the clusters are checked, rerun, etc. Nothing will be directly computed by this script so it can be ran in a login node.</p>

<p>When one step sends jobs to the cluster through <em>BatchJobs</em>, the user can exit R, log out, have a coffee, think about all the time saved thanks to <em>BatchJobs</em> and then open everything again and continue. No need to rerun everything, just load the libraries and the registry of the steps to check and continue.</p>

  </article>

</div>

    </div>

  </body>

</html>
